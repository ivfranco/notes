p45, imperative vs. declarative javascript:
the culprit here is "no abstraction" instead of being imperative

p74, "in order to find the value for a key, we first check the most recent segment’s hash map; if
the key is not present we check the second-most-recent segment, and so on."
... or store the epoch number as part of the index.

Branching factor of a B-tree is variable in a range, in Rust BTreeMap the range is [5, 11].

p87, multi-column indexes
One possible solution to multi-dimensional queries in relational databases is data cube, which is
standardized in ISO SQL 2018.

p95, Column-Oriented Storage
curiously akin to Entity Component System, in which components of objects rather than objects
themselves are stored together

p113, "if an attacker can get your application to decode an arbitrary byte sequence, they can
instantiate arbitrary classes, which in turn often allows them to do terrible things such as
remotely executing arbitrary code [6, 7]"
- [6]: CVE-2015-7501, Java, serializing / deserializing certain objects invokes methods through
    runtime reflection which can be exploited to run arbitrary code with forged payload
- [7]: CVE-2013-0156, Ruby, YAML.load occasionally evaluates serialized string which may include
    arbitrary code wrapped in `eval` function call
Both cases the vulnerability originated in lack of distinction between data and program in the
language and badly written serialization library, hence
> "For these reasons it’s generally a bad idea to use your language’s built-in encoding for
> anything other than very transient purposes"
doesn't necessarily apply to something like Bincode. In the Ruby case it's not even built-in
encoding, YAML is developed independently and had seen wide adoption since its early days.

[4] Frank McSherry, Michael Isard, and Derek G. Murray: “Scalability! But at What
COST?”
Hilarious paper presenting two main points:
1.  many workloads scales very well only because they are not optimized, after optimization the
    their performance no longer scales beyond ~10 cores
2.  many distributed algorithms despite scales very well is not more efficient than a simple
    single-threaded algorithm to begin with, not even with unbounded number of nodes
A reproduction conducted in 2019 (http://vaastavanand.com/blog/2019/Reproduction-Cost/) shows
that the single-threaded algorithm outperformed distributed algorithms even more due to improved
memory handling in Linux and better hardware

[23] Douglas B. Terry: “Replicated Data Consistency Explained Through Baseball”
Given 6 consistency model in partial order:
    Strong Consistency: See all previous writes.
    Bounded Staleness: See all “old” writes.
    Consistent Prefix: See initial sequence of writes.
    Monotonic Reads: See increasing subset of writes.
    Read My Writes: See all writes performed by reader.
    Eventual Consistency: See subset of previous writes.
where Strong Consistency is the absolute strongest,  Eventual Consistency is implied by all other
models, Bounded Staleness after a delay converges to Strong Consistency hence implies all others

The algorithm presented in the text is a variant of the dotted version vectors set (DVVSet)
operating on a single replica.
- https://gsd.di.uminho.pt/members/vff/dotted-version-vectors-2012.pdf
- https://github.com/ricardobcl/Dotted-Version-Vectors
The code (in Erlang) is barely documented, the company operating Riak went out of business in
2017, Amazon doesn't seem to be using version vectors any more since 2015

p208, Partitioning Secondary Indexes by Term
A discussion on this topic in Berkeley CS186 : https://www.youtube.com/watch?v=FNhJXpcc3I4,
partitioning indexes by terms can be a very bad idea since most human behaviors follow the Zipf
distribution: naturally 80% accesses will be on 20% terms

Common implementation of isolation levels
Read committed: 
    modification is performed on a different version of the store, only applied to the main store
    when commited
Snapshot isolation: 
    each transaction works on a snapshot version of the store identified by an always
    incrementing transaction id, modification is invisible to a transaction if
        1.  the modifying transaction is not committed yet when the current one started
        2.  modifying transaction has a later id
        3.  the modifying transaction aborted
    A background garbage collector periodically remove old values no longer visible to any
    ongoing and future transactions.
Serializable:
    1.  actual serial execution, one transaction at a time (became feasible recently)
    2.  locally shard the store then 1. may be executed concurrently
    3.  2PL: each object is protected by a read / write lock, in phase one the transaction tries to
        acquire all the locks of objects it may use, in phase two the transaction is executed and
        then locks released. To handle phantoms in addition to existing objects there must be lock
        on not yet existing objects (predicate locks) or ranges of them (index-range locks). 
    4.  Serializable snapshot isolation: the basic idea sketched by the text is superficially
        similar to that of software transactional memory (STM): on top of snapshot isolation,
        optimistically execute the transaction on a snapshot, commit only when the result is not
        invalidated by other current transactions; but STM is mentioned nowhere in the original
        paper [51], guess the description in the text must be over-simplified
    
[27] Michael Melanson: “Transactions: The Limits of Isolation”
As there's no commonly agreed interpretation to isolation levels provided by DBMS vendors, author
of this book wrote a tool to test them.

p242, "they also use B-trees (see “B-Trees” on page 79), they use an append-only/copy-on-write
variant that does not overwrite pages of the tree when they are updated, but instead creates a
new copy of each modified page"
One such persistent binary search tree is introduced in Introduction to algorithms, problems 13-1

[21] Salvatore Sanfilippo: “A Few Arguments About Redis Sentinel Properties and Fail Scenarios”
A Redis leader, configured not to persist data on disk, crashed and restarted with an empty
store, yet managed to restart so fast that the watcher instance (Redis Sentinel) hadn't detected
the failure, as a result all replica synchronized with the leader and wiped their database. The
Redis developer (author of the blog) talked about how to prevent future accidents by a version
tag ("runid"), which is now implemented.

[90] Bowen Alpern and Fred B. Schneider: “Defining Liveness”
let 
    S be the set of program states
    S^ω be the set of infinite sequences of program states
    σ ∈ S^w be executions of program
    S^* be the set of finite sequences of program states
    σ_i ∈ S^*, the length i prefix of σ, be partial executions
    σ |= P when an execution σ of a program is in property P
safety property is defined as
    ∀σ∈S, ~(σ |= P) => ∃i>=0, ∀β∈S^ω, ~(σ_i β |= P)
that is, when an execution doesn't satisfy a safety property, there must be a point in the execution
that, after which the safety property is irredeemable: no further actions may make the execution
satisfy P ever again
liveness property is defined as
    ∀α∈S^*, ∃β∈S^ω, αβ |= P
that is, for any execution of the program, there's always a (maybe infinite) sequence of actions
that, after which the execution satisfies the liveness property P, or the liveness property
will "eventually happen"

[46] Nelson Minar: “Leap Second Crashes Half the Internet”
a bug in adjtimex(2) set some timer in the kernel one second ahead of others, as a consequence
clock_nanosleep shorter than 1 second will return immediately, turning certain loops into spin
locks which saturated CPU

[6] Maurice P. Herlihy and Jeannette M. Wing: “Linearizability: A Correctness Condition for
Concurrent Objects”
An operation invocation is a four tuple:
    (x, op, args, A), or by the notation of the paper,
    <x op(args*) A>
where
    x:      the object name
    op:     an operation name
    args:   a sequence of argument values
    A:      a process name
A response is a four tuple:
    <x term(res*) A>
where
    x:      the object name
    term:   a termination condition
    res:    a sequence of results
    A:      a process name
A response matches an invocation if their object names agree and their process names agree.
An event is either an invocation or a response
A history H is a sequence of events
complete(H) is the maximum subsequence of H that all invocations has matching responses
A history H is sequential if 
    1.  H starts with an invocation
    2.  except for the final invocation, each invocation is followed immediately by a matching
        response each response is followed immediately by a matching invocation
A process sub-history H|P is the subsequence of all events in H whose process names are P
An object sub-history H|x is similarly defined
A history is well-formed if for all P, H|P is sequential
Two histories H and H' are equivalent if
    ∀P, H|P = H'|P
A sequential specification for an object is a prefix-closed set of single-object sequential
histories for that object
A history H is legal if:
    1.  H is sequential
    2.  H|x for all x belongs to the sequential specification for x
An operation 
    [q inv/res A]
is a pair of matching invocation and the next matching response
An operation e0 lies within another operation e1 if inv(e1) precedes inv(e0) and res(e0) precedes
res(e1)
A history H induces an partial order <_H on operations:
    e0 <_H e1 if res(e0) precedes inv(e1) in H
if H is sequential, <_H is a total order
A history H is linearizable if it can be extended (by appending zero or more response events) to
some history H’ such that:
    1.  complete(H') is equivalent to some legal sequential history S
    2.  <_H ⊆ <_S
in other words, the completed operations in the history may be seen as sequentially executed while
preserving all the orderings and logical correctness.
Linearizability is local:
    H is linearizable iff H|x is linearizable for all object x
Linearizability is non-blocking:
    Let inv be an invocation of a total operation. If <x inv P> is a pending invocation in a
    linearizable history H, then there exists a response <x res P> such that H . <x res P> is
    linearizable.

[56] Leslie Lamport: “Time, Clocks, and the Ordering of Events in a Distributed System”
Define "happens before" relation -> as the transitive closure of:
    1.  a -> b if event a happens before b on the same process
    2.  a -> b if a sends a message and b is the receipt 
(in this paper events are instant, not a period of time)
naturally -> is a superset of causal dependencies
A clock C is a function that maps events to numbers, the correctness of a clock is defined as:
    a -> b implies C(a) < C(b)
An intuitive way to satisfy the correctness condition is, set a counter in each process that:
    1.  increment the counter between each two successive events in that process
    2.  tag each message with the current counter, upon receiving the message advance the counter to
        a number greater than the received number in the message
Assign an arbitrary total order to processes, define a total order => over tuples of events and
processes that:
    (C(a), Pi) => (C(b), Pj) iff C(a) < C(b) | (C(a) = C(b) & Pi < Pj
a simple proof of concept:
./distributed_system/lamport_timestamp
Without errors, any program that can be described as a (possibly infinite) state machine
    F: C x S -> S
where
    C: set of commands 
    S: set of states
    F: a function computes the next state from a command and the current state 
may be correctly implemented as a distributed system with Lamport timestamp and message
broadcasting: the order of comments is ensured by the total order =>.
Later half of the paper is dedicated on replacing the counter in Lamport timestamp by physical
times

[68] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson: “Impossibility of Distributed
Consensus with One Faulty Process”
helpful contents:
    https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility/
    https://www.youtube.com/watch?v=Vmlj-67aymw
---NOTE START---
Let a consensus protocol P be an asynchronous system of N processes (N >= 2)
Let xp, yp be input and output register with value in {0, 1, b}, initially yp = b, if yp ∈ {0, 1}
the process is in decision states, once written output register cannot be modified (stricter than
Integrity of consensus model)
P acts deterministically according to a transition function 

A message is a tuple (p, m) where
    p:  a process id
    m:  a message from a fixed universe M

A message system supports two operations:
    send(p, m): places (p, m) in the message buffer
    receive(p): deletes _some_ message (p, m) from message buffer and return m, or delete none
                and return null

The message system is nondeterministic, the only guarantee is if there's a message (p, m) in message
buffer, calling receive(p) will eventually return m (after a finite yet unbounded number of null)

A configuration C of the system consists of the initial states of processes and the content of the
message buffer

A event (p, m) deterministically changes the internal state of a process
A step C -> C' consists of a primitive step by a single process p:
    1.  let m = receive(p) // m may be null
    2.  apply the event (p, m) to C

A schedule σ of C is a finite or infinite sequence of events that can be applied in turn, starting
from C, let σ(C) be the resulting configuration
A configuration reachable from the initial state is called accessible

Lemma 1: schedules are communicative
    from some configuration C, let schedules σ1, σ2 lead to configurations C1 and C2, if the sets of
    processes taking steps in σ1 and σ2 are disjoint, then σ2(C1) = σ1(C2)

A configuration C has decision value v if some process p is in a decision state v
A consensus protocol is partially correct if it satisfies two conditions:
    1.  no accessible configuration has more than one decision value
        (Uniform agreement in consensus model)
    2.  for each v ∈ {0, 1} there is a accessible configuration with decision value v
        (Validity in consensus model)

A run is admissible if there's at most one faulty (non-responding for some period) process at a time
A run is deciding provided that some (protocol specific) process reaches a decision state
A consensus protocol is totally correct in spite of one fault if
    1.  it is partially correct
    2.  every admissible run is (finitely) deciding 
        (Termination in consensus model)

Theorem 1: No consensus protocol is totally correct in spite of one fault

Let V be the set of decision values of reachable configurations from C, C is bivalent if |V| = 2,
bivalent configuration by definition is not deciding

Two configurations are adjacent if they differ in only the initial value xp of a process p 

Lemma 2: totally correct protocol P has a bivalent initial configuration
    Assume not, by partial correctness there must be initial configurations with decision value 0
    or 1. Any configuration can be reached from adjacent configurations from another, configuration,
    hence there must be two adjacent initial configurations C0 and C1 with decision values 0 and 1.
    Consider a admissible deciding run σ on C0 in which process p takes no steps (that is, p is the
    faulty process), σ can be applied on C1, σ(C0) = σ(C1), either C0 or C1 is bivalent.

Lemma 3: let C be a bivalent configuration of P, let e = (p, m) be an event applicable to C. let S
be the set of reachable configurations from C without applying e, let 
    S' = e(S) = {e(E) | E ∈ S, e applicable to E}
then S' also contains a bivalent configuration.
    For all E ∈ S, e is applicable to E (the paper assumes that if an event is applicable to a
    process, it's applicable regardless the internal state).

    Assume S' contains no bivalent configurations. let E0, E1 be configurations reachable from C
    with decision value 0 and 1 (exist by bivalent-ness of C). If Ei ∈ S, let Fi = e(Ei) ∈ S',
    otherwise e was applied in reaching Ei, there's Fi ∈ S' from which Ei is reachable. Either case,
    Fi is i-valent (all reachable configuration from Fi has decision value i). S' must have both
    1-valent and 0-valent configurations.

    C and D are neighbors iff 
        ∃e, C = e(D) or D = e(C)
    there must be C0, C1 where Di = e(Ci) ∈ S' are i-valent:
        without loss of generality let e(C) be 0-valent, by the argument above there must be some σ
        that e ∉ σ and e(σ(C)) 1-valent, one step e' in σ has e'(C0) = C1

    let e' = (p', m'), C1 = e'(C0),
        If p != p', by Lemma 1, D1 = e(e'(C0)) = e'(e(C0)) = e'(D0), but D0 is 0-valent, all
        configurations reachable from D0 must be 0-valent too

        If p = p', let σ be a finite deciding run from C0 in which p takes no steps (must exist
        by total correctness). Let A = σ(C0), by Lemma 1 σ is applicable on D0 and D1, by
        valent-ness Ei = σ(Di) has the same valent-ness to Di, again by Lemma 1 e(A) = E0 and
        e(e'(A)) = E1. A is bivalent (not deciding).
    
Any deciding run must be univalent, in a totally correct protocol there must be a step that changes
the initial bivalent configuration (partial correctness 2) to a univalent configuration, but there's
always possible to avoid such a step in an admissible run.

Put processes in an imaginary queue (cause the execution order in asynchronous model can be
arbitrary), message buffer in the configuration is ordered according to the time the messages
were sent (again unspecified in asynchronous model). In each stage:
    Each process may take steps, the stage ends when the first process in the queue takes a step 
    in which its earliest message (if any at the start of the stage) is received.

    At the end of a stage, the first process in the queue is moved to the back of the queue.

The run is admissible since each process may receive infinite messages given infinite stages, when
starting from a bivalent initial configuration (must exist by Lemma 2):
    let C be the bivalent configuration at the start of the stage, let 
        m be the earliest message to p in C's message buffer or null otherwise
        e = (p, m)

    By Lemma 3, there's a bivalent configuration C' reachable from C by a schedule σ in which e is
    the last event, σ matches the definition of a stage and C' = σ(C) is bivalent.

The run is admissible but not finitely deciding, therefore no protocol is totally correct in spite
of one fault.
---NOTE END---

p365, Consensus means:
    1.  Uniform agreement: no two nodes decide differently.
        The fundamental idea behind consensus.
    2.  Integrity: no node decides twice.
        The fundamental idea behind consensus.
    3.  Validity: if a node decides value v, then v was proposed by some node.
        To rule out trivial solutions (e.g. always return null)
    4.  Termination: every node that does not crash eventually decides some value.
        Formalizes fault tolerance: functioning nodes must make progress even if other nodes fail

p365, consensus system model usually: 
    1.  assumes a crashed node never respond nor come back
    2.  assumes no Byzantine faults (adversary nodes)
    3.  decide on a sequence of values instead of a single one at a time

Systems of record 
    -   Normalized
    -   correct by definition (source of truth in the system)
Derived data systems 
    -   denormalized
    -   redundant

[1] Jeffrey Dean and Sanjay Ghemawat: “MapReduce: Simplified Data Processing on
Large Clusters”
---NOTE START---
MapReduce is the combination of 4 functions:
    1.  read :: Data -> [(RawKey, RawValue)]
        extract the data from the database / file system. 
        library defined or user defined
    2.  map :: (Eq IKey) => (RawKey, RawValue) -> [(IKey, IValue)]
        maps raw (key, value) pair to multiple intermediate (key, value) pair
        the intermediate keys must be able to be compared for equality, (the Google implementation
        requires them to be ordered)
        user defined
    3.  reduce :: (IKey, [IValue]) -> Output
        aggregates all intermediate values under the same intermediate key
        user defined
    4.  output :: Output -> IO ()
        output the result to the file system
        library defined or user defined
Every single type (maybe except Data) in the functions must be serializable as they are buffered in
local storage, output to a file or passed over network.

An execution flow of Google's implementation of MapReduce:
    1.  the compiled executable is distributed to multiple worker machines, one of them waiting for
        user inputs (the master)
    2.  when invoked, the master partitions the input data on remote file system to pieces of ~64MB,
        one or more pieces is assigned to remote Map workers
    3.  the Map workers extract data from the file system / database (by some remote access
        protocol?), apply map function to the data and buffer them in local storage
    4.  the master notifies a Reduce worker when the intermediate data are complete, Reduce worker
        reads the data by RPC from the Map workers, sort the data by intermediate keys and apply
        reduce function on each collection of values grouped by key
    5.  the output is appended to an output file in the global file system

MapReduce by Google has a single-leader design: the master process periodically pings all the
workers, reset their state and output when workers are considered failed.

The model described in the paper is vulnerable to read skews, maybe it's handled by the file system
(GFS apparently) or read skew is insignificant in their use cases.
---NOTE END---

[19] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: “The Google File System”
---NOTE START---
Optimized for
    - frequent component failure, a norm when there's thousands of commodity grade nodes in the
        system, error detection and automatic failure recovery are crucial to the system
    - large files (> 100MB, multi-GB files are common), in contrary to majorly small (~4KB) files in
        personal computing environment
    - large sequential read and write (from MapReduce instances), in contrary to random read / write
        in personal computing environment
    - concurrent append (multiple reducers appending to the same output file)
    - high sustained bandwidth instead of latency (MapReduce is a batch process)

In addition to normal create, delete, open, close, read, write operations, GFS supports
    - snapshot: create a copy of a file or a directory at low cost
    - record append: atomic concurrent append to files

Architecture:
    - running as a user land process on top of linux file system 
    - files are divided to fixed-size chunks, each chunk has a unique 64-bit chunk handle, chunks
      are replicated on multiple machines
    - a single master instance manages all the metadata, sends heartbeats to chunk servers
      periodically (pre-designated single leader)
    - no file cache in client or chunk servers (workloads are large sequential reads / writes), but
      file metadata is cached in client
    
Workflow of a read:
    1.  client -> GFS master: file name, chunk index (calculated from the fixed size of chunks)
    2.  GFS master -> client: chunk handle and chunk location translated from file name and index
    3.  client -> GFS chunk server: chunk handle, chunk location and operation
    4.  GFS chunk server -> client: requested data

Chunks are large (64MB) and lazily allocated by the linux file system. For large sequential reads /
writes a large chunk size reduces the required number of communications between client and master,
in the cost of lower granularity hence more hot spots, one such incident was resolved by increasing
replication factor and introducing random delay to application launch.

GFS metadata:
    - file and chunk namespaces (persistent, replicated)
    - mapping from file to chunks (persistent, replicated)
    - locations of replicas (volatile, synced with chunk servers at master startup, join of a new
      chunk server and heartbeat)
    
The consistency model of GFS is relaxed: concurrent appends may cause staled read, appends are
applied in at-least-once semantics, the result file may contain padding, duplicated or interleaved
writes. It's the client application's responsibility to write self-validating, self-identifying
data.

A log-structured store of file system operation logs are persisted on master and multiple remote
locations. File system state is restored after reboot by replaying the operation logs. Log data is
compressed to a file system state checkpoint after reaching a certain size.

GFS consistency model:
    - file namespace mutations (creation, rename, etc.) are atomic and linearizable
    - consistency of file content immediately after mutations depends on operations:
        // consistent: all clients see the same data across all replica
        // defined: mutation is observable in its entirety or more
        - serial successful write:      consistent, defined
        - concurrent successful write:  consistent, undefined
        - serial successful append:     inconsistent, defined
        - concurrent successful append: inconsistent, defined
        - failed mutation:              inconsistent, undefined

After a sequence of successful mutations the resulting file system is defined and eventually
consistent. The window of inconsistency originates from client cache and stale replica.

Chunk corrupted by failed mutation on files will be detected by checksum and replaced by a replica.
A chunk is lost only if all replicas (3 by default) are lost, user requiring lost chunk receives
error instead of corrupted data.

Possible client-side solution to the inconsistency of GFS:
    - append checksum to each write, verify on read
    - assign unique id to records, filter duplicate appends

Workflow of a write:
    1.  client -> master: ask which chunk server holds the lease and its replica
        master -> chunk servers: grant lease to a replica if no one has it
    2.  master -> client: identity of replicas, one of them is primary
    3.  client -> chunk servers: push mutation operation in arbitrary order
        chunk servers -> client: acknowledgement
    4.  client -> primary: write request
        primary assign serial number to mutations
        primary applies mutation locally
    5.  primary -> replicas: forward write request
        replicas applies mutations in the same order as the primary
    6.  replicas -> primary: operation result
    7.  primary -> client: aggregated operation result
        retry steps 3-7 a few times on failure of any replica, then retry from beginning
Data is piped through TCP in one pass, transferring B bytes of data to R replicas costs:
    denote
        T: transfer rate
        L: TCP handshake latency
    B/T + RL
ideally 1MB can be distributed to all replicas in 80ms

Atomic record appends 
    - performs at-least-once file append 
    - if the current chunk cannot accommodate the record:
        1.  allocate a new chunk 
        2.  tell replicas to do the same
        3.  tell client to retry
    - replicas are not byte-wise identical (defined but inconsistent)

Snapshot is implemented as copy-on-write: initially only the metadata is duplicated pointing to the
same source chunks, which are duplicated on modifications. The new chunks are created on the same
server as the source chunks, means snapshots cannot be used in place of backups.

Data is forwarded from each replica to its closest neighbor instead of being distributed in a
tree-shaped topology. Data is pipelined through TCP, is immediately forwarded on receiving.

GFS doesn't support hard or symbolic links, nor does it have per-directory data structure that must
be locked on file creation. All file paths are stored in a single structure with prefix compression,
multiple files may be created concurrently in the same directory as there's no inode: file creation
requires only read lock of all ancestors (so the ancestors cannot be removed) and write lock of the
file path.

Location of a new chunk is based on 3 measurements:
    1.  disk space utilization: prioritize less full instance
    2.  recent creates: evenly distribute recent chunk create (hence the following heavy writes)
    3.  spread replicas across racks
GFS master re-replicates chunks in order of priority calculated from:
    - how far is it from the replication goal (replication factor)
    - file liveliness of the chunk
    - ongoing client access
Replicas are rebalanced by the master periodically among chunk servers according to criteria similar
to chunk creation.

Chunks of deleted files and from failed file creation are GCed by a background process on master.
Chunks of deleted files are first renamed to a special hidden name then GCed 3 days later. The paper
claim that lazy storage reclamation by GC is superior to edger deletion as:
    - GC provides a uniformed solution to dangling chunks created by failed operations
    - GC is done in batches as a routine background process when master is free, network cost is
        amortized
    - accidentally deleted files could still be recovered 

GFS master issues a new version number of a chunk when granting a lease to the chunk, the version
number is propagated to all replicas of the chunk. A chunk replica for any reason with a version
number lower than master is treated as failed and GCed later.

Availability is ensured by:
    - fast recovery: client retry, states can be recovered in seconds, no distinction between normal
      and abnormal termination
    - chunk replication and checksum (detection only, no ECC)
    - duplicated masters: a new master can be launched in seconds from operation log on a different
      machine, read-only "shadow" masters

Data integrity on chunk servers is maintained by checksum, both on file access and periodically.
---NOTE END---

p457, "Event sourcing makes it easier to evolve applications over time, helps with debugging by
making it easier to understand after the fact why something happened"
the basic idea behind redux and to some extent the entire React framework, the "biggest downside"
which is inconsistency doesn't exist in frontend world as javascript is single-threaded.

[22] Diego Ongaro and John K. Ousterhout: “In Search of an Understandable Consensus
Algorithm (Extended Version)”
a demonstration of the election process in raft protocol can be found in
    ./distributed_system/raft_election

[77] Alan Woodward and Martin Kleppmann: “Real-Time Full-Text Search with
Luwak and Samza”
By "indexing queries" the author meant to extract logical clauses from queries and match it against
a logical clause built from the documents in the stream.

[90] Viktor Klang: “I’m coining the phrase ‘effectively-once’ for message processing with
at-least-once + idempotent operations”

p518, "For example, you could generate a unique identifier for an operation (such as a UUID) and
include it as a hidden form field in the client application" Which is a bad idea: even if the user
won't manually manipulate the form data, there's non negligible chance one of the dozen extensions
the user installed on their browser will mess over the form data, sometimes even adversarially. No
data submitted by the user should be trusted.  The UUID should be generated at server side as part
of the initial client state and verified on its way back to the server.
